{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-pKMcCtXcNRUdOg-3PHOLl1q2p4Ia8kC","timestamp":1749119839034}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import random\n","import json\n","import numpy as np\n","from google.colab import files\n","\n","num_samples = 5000  # ìƒì„±í•  ë°ì´í„° ê°œìˆ˜\n","data = []\n","\n","#ì˜¬ë°”ë¥¸ ê²½ìš°\n","for _ in range(num_samples):\n","    strokes = []\n","    x_start = random.uniform(40, 80)\n","    y_start = random.uniform(80, 100)\n","\n","    path1=[]\n","    for i in range(5):\n","      if i == 0 :\n","        path1.append([x_start, y_start, round(0.1 * (i+1), 1)])\n","      else :\n","        new_x = path1[-1][0] + random.uniform(-1,1)\n","        new_y = path1[-1][1] - random.uniform(10,19)\n","        path1.append([new_x, new_y, round(0.1 * (i+1),1)])\n","    strokes.append({\"stroke_id\":1, \"path\":path1})\n","\n","    data.append({\n","        \"character\" : \"ã…£\",\n","        \"strokes\" : strokes,\n","        \"label\" : \"correct\"\n","    })\n","\n","#ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê²½ìš°\n","for _ in range(num_samples):\n","    strokes = []\n","    x_start = random.uniform(40, 80)\n","    y_start = random.uniform(0, 20)\n","\n","    path1=[]\n","    for i in range(5):\n","      if i == 0 :\n","        path1.append([x_start, y_start, round(0.1 * (i+1), 1)])\n","      else :\n","        new_x = path1[-1][0] + random.uniform(-1,1)\n","        new_y = path1[-1][1] + random.uniform(10,19)\n","        path1.append([new_x, new_y, round(0.1 * (i+1),1)])\n","    strokes.append({\"stroke_id\":1, \"path\":path1})\n","\n","    data.append({\n","        \"character\" : \"ã…£\",\n","        \"strokes\" : strokes,\n","        \"label\" : \"incorrect\"\n","    })\n","\n","# JSON íŒŒì¼ ì €ì¥\n","data_filename = \"generated_ã…£_dataset.json\"\n","with open(data_filename, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(data, f, ensure_ascii=False, indent=4)\n","\n","files.download(data_filename)\n","print(f\"íŒŒì¼ {data_filename} ìƒì„± ì™„ë£Œ!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":32},"id":"Vj44U8bQ_Gtg","executionInfo":{"status":"ok","timestamp":1739931743570,"user_tz":-540,"elapsed":821,"user":{"displayName":"â€ì„œì •í›ˆ[í•™ìƒ](ê³µê³¼ëŒ€í•™ ì‚°ì—…ê²½ì˜ê³µí•™ê³¼)","userId":"13642878406273598628"}},"outputId":"0b7ab1cd-f5d3-444c-95c3-9723681cbd87"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_cf021b40-2445-4539-88e4-81ffe4e64a9f\", \"generated_\\u3163_dataset.json\", 10002482)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["íŒŒì¼ generated_ã…£_dataset.json ìƒì„± ì™„ë£Œ!\n"]}]},{"cell_type":"code","source":["import json\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Masking, Bidirectional\n","from sklearn.model_selection import train_test_split\n","\n","# ë°ì´í„°ì…‹ ë¡œë“œ\n","with open(\"/content/generated_ã…£_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n","    data = json.load(f)\n","\n","# ğŸ“Œ 2ï¸âƒ£ ë°ì´í„°ì…‹ ë³€í™˜\n","X, y = [], []\n","\n","for sample in data:\n","    all_strokes = []\n","\n","    for stroke in sample[\"strokes\"]:\n","        all_strokes.extend(stroke[\"path\"])  # ëª¨ë“  strokeë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨\n","\n","    X.append(np.array(all_strokes))  # (N, 3) í˜•íƒœ\n","    y.append(1 if sample[\"label\"] == \"correct\" else 0)  # correct: 1, incorrect: 0\n","\n","# ğŸ“Œ 3ï¸âƒ£ Padding: LSTM ì…ë ¥ í¬ê¸° í†µì¼\n","max_length = max(len(p) for p in X)  # ê°€ì¥ ê¸´ sequence ì°¾ê¸°\n","X_padded = np.zeros((len(X), max_length, 3))  # (ë°ì´í„° ê°œìˆ˜, ìµœëŒ€ ê¸¸ì´, 3)\n","for i, path in enumerate(X):\n","    X_padded[i, :len(path), :] = path  # íŒ¨ë”© ì ìš©\n","\n","# ğŸ“Œ 4ï¸âƒ£ Independent Min-Max Normalization\n","def min_max_normalize(data):\n","    for i in range(2):  # x, y ì¢Œí‘œ ì •ê·œí™”\n","        min_val = np.min(data[:, :, i])\n","        max_val = np.max(data[:, :, i])\n","        data[:, :, i] = (data[:, :, i] - min_val) / (max_val - min_val + 1e-8)  # ì‘ì€ ê°’ ì¶”ê°€í•˜ì—¬ 0-ë¶„ëª¨ ë°©ì§€\n","    return data\n","\n","X_padded = min_max_normalize(X_padded)\n","\n","# ğŸ“Œ 5ï¸âƒ£ Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_padded, np.array(y), test_size=0.2, random_state=42)\n","\n","# ğŸ“Œ 6ï¸âƒ£ LSTM ëª¨ë¸ ì •ì˜\n","model = Sequential([\n","    Masking(mask_value=0.0, input_shape=(max_length, 3)),  # íŒ¨ë”©ëœ ë¶€ë¶„ ë¬´ì‹œ\n","    Bidirectional(LSTM(128, return_sequences=True)),  # ì–‘ë°©í–¥ LSTM\n","    LSTM(64, return_sequences=True),\n","    LSTM(32),\n","    Dense(16, activation=\"relu\"),\n","    Dense(1, activation=\"sigmoid\")  # Binary classification (correct/incorrect)\n","])\n","\n","model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","\n","# ğŸ“Œ 7ï¸âƒ£ ëª¨ë¸ í•™ìŠµ\n","model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n","\n","# ğŸ“Œ 8ï¸âƒ£ ëª¨ë¸ ì €ì¥\n","model.save(\"/content/drive/MyDrive/lstm_model.h5\")\n","np.save(\"/content/drive/MyDrive/max_length.npy\", max_length)  # max_lengthë„ ì €ì¥\n","\n","print(\"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ë° ì €ì¥ë¨!\")"],"metadata":{"id":"a3lKjVAt_tRK","executionInfo":{"status":"ok","timestamp":1739931858619,"user_tz":-540,"elapsed":62940,"user":{"displayName":"â€ì„œì •í›ˆ[í•™ìƒ](ê³µê³¼ëŒ€í•™ ì‚°ì—…ê²½ì˜ê³µí•™ê³¼)","userId":"13642878406273598628"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"43f09d1a-789d-4ef3-d115-5650ee78c242"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/masking.py:47: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 41ms/step - accuracy: 0.9100 - loss: 0.2372 - val_accuracy: 1.0000 - val_loss: 4.4378e-04\n","Epoch 2/5\n","\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 3.2814e-04 - val_accuracy: 1.0000 - val_loss: 1.5424e-04\n","Epoch 3/5\n","\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.2935e-04 - val_accuracy: 1.0000 - val_loss: 7.9147e-05\n","Epoch 4/5\n","\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 6.9832e-05 - val_accuracy: 1.0000 - val_loss: 4.8326e-05\n","Epoch 5/5\n","\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.3191e-05 - val_accuracy: 1.0000 - val_loss: 3.2516e-05\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ë° ì €ì¥ë¨!\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","\n","# ğŸ“Œ 1ï¸âƒ£ ëª¨ë¸ ë° max_length ë¶ˆëŸ¬ì˜¤ê¸°\n","model_path = \"/content/drive/MyDrive/lstm_model.h5\"\n","max_length_path = \"/content/drive/MyDrive/max_length.npy\"\n","\n","model = load_model(model_path)\n","max_length = int(np.load(max_length_path))\n","\n","print(\"âœ… ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!\")\n","\n","# ğŸ“Œ 2ï¸âƒ£ ì˜ˆì¸¡ í•¨ìˆ˜\n","def predict_sequence(input_strokes):\n","    input_path = []\n","\n","    for stroke in input_strokes:\n","        input_path.extend(stroke[\"path\"])  # ëª¨ë“  stroke í•©ì¹˜ê¸°\n","\n","    input_path = np.array(input_path)\n","\n","    # Padding\n","    input_padded = np.zeros((1, max_length, 3))\n","    input_padded[0, :len(input_path), :] = input_path\n","\n","    # Independent Min-Max Normalization (for single input)\n","    for i in range(2):\n","        min_val = np.min(input_padded[:, :, i])\n","        max_val = np.max(input_padded[:, :, i])\n","        input_padded[:, :, i] = (input_padded[:, :, i] - min_val) / (max_val - min_val + 1e-8)\n","\n","    # Prediction\n","    pred = model.predict(input_padded)\n","    return \"correct\" if pred[0][0] > 0.5 else \"incorrect\", pred\n","\n","# ğŸ“Œ ğŸ”¥ ì˜ˆì œ í…ŒìŠ¤íŠ¸\n","test_sample = {\n"," \"strokes\": [\n","            {\n","                \"stroke_id\": 1,\n","                \"path\": [\n","                    [\n","                        79.789472866258,\n","                        3.6633368904884267,\n","                        0.1\n","                    ],\n","                    [\n","                        78.8507700307559,\n","                        20.488036257179438,\n","                        0.2\n","                    ],\n","                    [\n","                        78.61341748679581,\n","                        37.72585989284296,\n","                        0.3\n","                    ],\n","                    [\n","                        78.62261716790583,\n","                        56.23775787137843,\n","                        0.4\n","                    ],\n","                    [\n","                        79.1530780787082,\n","                        71.5094578993419,\n","                        0.5\n","                    ]\n","                ]\n","            }\n"," ]\n","}\n","result_label, pred_value = predict_sequence(test_sample[\"strokes\"])\n","print(\"Predicted result:\", result_label)\n","print(\"Prediction probability:\", pred_value)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8BCaHzVVoKy","executionInfo":{"status":"ok","timestamp":1739932789216,"user_tz":-540,"elapsed":1021,"user":{"displayName":"â€ì„œì •í›ˆ[í•™ìƒ](ê³µê³¼ëŒ€í•™ ì‚°ì—…ê²½ì˜ê³µí•™ê³¼)","userId":"13642878406273598628"}},"outputId":"c587d942-3b15-4c7d-eb2c-1b5904bf96df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["âœ… ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!\n","\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796ms/step\n","Predicted result: incorrect\n","Prediction probability: [[1.6459422e-05]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0jvGgHHbYKzy"},"execution_count":null,"outputs":[]}]}