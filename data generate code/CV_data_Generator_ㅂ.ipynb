{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1dVEbO4XQoraPxBwA3JiUh6tMJtJmU986","timestamp":1749119648506}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":32},"id":"_LXppxwrAi6J","executionInfo":{"status":"ok","timestamp":1739936035300,"user_tz":-540,"elapsed":4285,"user":{"displayName":"â€ì„œì •í›ˆ[í•™ìƒ](ê³µê³¼ëŒ€í•™ ì‚°ì—…ê²½ì˜ê³µí•™ê³¼)","userId":"13642878406273598628"}},"outputId":"7fc73415-f01b-41f4-db9c-edd34616710a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_ce9c848f-05f1-45c3-a06c-db0d4ec1c4cf\", \"generated_\\u3142_dataset.json\", 51580918)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["íŒŒì¼ generated_ã…‚_dataset.json ìƒì„± ì™„ë£Œ!\n"]}],"source":["import random\n","import json\n","import numpy as np\n","from google.colab import files\n","\n","num_samples = 20000  # ìƒì„±í•  ë°ì´í„° ê°œìˆ˜\n","data = []\n","\n","for _ in range(num_samples):\n","    case_type = random.choice([\"correct\", \"incorrect_2\", \"incorrect_3\", \"incorrect_4\"])\n","    strokes = []\n","\n","    if case_type == \"correct\":  # ì˜¬ë°”ë¥¸ ã…‚ (4íš)\n","        x_start = random.uniform(10, 40)\n","        y_start = random.uniform(80, 100)\n","\n","        # ì²« ë²ˆì§¸ ì„¸ë¡œíš (ì™¼ìª½)\n","        path1 = []\n","        for i in range(5):\n","            if i == 0 :\n","              path1.append([x_start, y_start, 0.1])\n","            else :\n","              path1.append([path1[-1][0] + random.uniform(-1, 1), path1[-1][1] - random.uniform(10, 15), round(0.1 * (i+1), 1)])\n","        strokes.append({\"stroke_id\": 1, \"path\": path1})\n","\n","        # ë‘ ë²ˆì§¸ ì„¸ë¡œíš (ì˜¤ë¥¸ìª½)\n","        x_start2 = x_start + 15\n","        path2 = []\n","        for i in range(5):\n","          if i == 0 :\n","            path2.append([x_start2, y_start, 0.1])\n","          else :\n","            path2.append([path2[-1][0] + random.uniform(-1, 1), path2[-1][1] - random.uniform(10, 15), round(0.1 * (i+1), 1)])\n","        strokes.append({\"stroke_id\": 2, \"path\": path2})\n","\n","        # ì„¸ ë²ˆì§¸ ê°€ë¡œíš (ìœ„ìª½)\n","        path3 = []\n","        for i in range(5):\n","          if i == 0 :\n","            path3.append([x_start, y_start - 20, 0.1])\n","          else :\n","            path3.append([path3[-1][0] + random.uniform(10, 15), path3[-1][1] + random.uniform(-1, 1), round(0.1 * (i+1), 1)])\n","        strokes.append({\"stroke_id\": 3, \"path\": path3})\n","\n","        # ë„¤ ë²ˆì§¸ ê°€ë¡œíš (ì•„ë˜ìª½)\n","        path4 = []\n","        for i in range(5):\n","          if i == 0 :\n","            path4.append([x_start, path1[-1][1], 0.1])\n","          else :\n","            path4.append([path4[-1][0] + random.uniform(10, 15), path4[-1][1] + random.uniform(-1, 1), round(0.1 * (i+1), 1)])\n","        strokes.append({\"stroke_id\": 4, \"path\": path4})\n","\n","    # ì˜ëª»ëœ case1 (2íš)\n","    elif case_type == \"incorrect_2\":\n","        # í•˜ë‚˜ì˜ ê°€ë¡œíš + ì„¸ë¡œíšë§Œ ì‚¬ìš©\n","        x_start = random.uniform(10, 60)\n","        y_start = random.uniform(80, 100)\n","        #ì²«ë²ˆì§¸ íš(ì™¼ìª½ ì„¸ë¡œì„ )\n","        path1 = []\n","        for i in range(5):\n","          if i == 0 :\n","            path1.append([x_start, y_start, 0.1])\n","          else :\n","            path1.append([path1[-1][0] + random.uniform(-1, 1), path1[-1][1] - random.uniform(10, 19), round(0.1 * (i+1), 1)])\n","        strokes.append({\"stroke_id\": 1, \"path\": path1})\n","\n","        # ë‘ë²ˆì§¸ ì„¸ë¡œíšë¶€í„° ì‹œì‘ (ì˜¤ë¥¸ìª½)\n","        x_start2 = x_start + 20\n","        path2 = []\n","        for i in range(5):\n","            if i == 0 :\n","              path2.append([x_start2, y_start, 0.1])\n","            elif i == 1 :\n","              new_x = path2[-1][0] + random.uniform(-1,1)\n","              new_y = path2[-1][1] - random.uniform(50,70)\n","              path2.append([new_x, new_y, round(0.1 * (i + 1), 1)])\n","            elif i == 2 :\n","              new_x = path2[-1][0] - 20 + random.uniform(-1,1)\n","              new_y = path2[-1][1] + random.uniform(-1,1)\n","              path2.append([new_x, new_y, round(0.1 * (i + 1), 1)])\n","            elif i == 3 :\n","              new_x = path2[-1][0] + random.uniform(-1,1)\n","              new_y = path2[-1][1] + random.uniform(10, 20)\n","              path2.append([new_x,new_y, round(0.1 * (i+1),1)])\n","            else :\n","              new_x = path2[-1][0] + 20 + random.uniform(-1,1)\n","              new_y = path2[-1][1] + random.uniform(-1,1)\n","              path2.append([new_x,new_y, round(0.1 * (i+1),1)])\n","        strokes.append({\"stroke_id\": 2, \"path\": path2})\n","\n","    # ì˜ëª»ëœ case2 (2íš)\n","    elif case_type == \"incorrect_3\":\n","        x_start = random.uniform(10, 60)\n","        y_start = random.uniform(80, 100)\n","\n","        # ì²«ë²ˆì§¸ íš\n","        path1 = []\n","        for i in range(5):\n","            if i == 0 :\n","              path1.append([x_start, y_start, 0.1])\n","            elif i == 1 :\n","              new_x = path1[-1][0] + random.uniform(-1,1)\n","              new_y = path1[-1][1] - random.uniform(50,70)\n","              path1.append([new_x, new_y, round(0.1 * (i + 1), 1)])\n","            elif i == 2 :\n","              new_x = path1[-1][0] + 20 + random.uniform(-1,1)\n","              new_y = path1[-1][1] + random.uniform(-1,1)\n","              path1.append([new_x, new_y, round(0.1 * (i + 1), 1)])\n","            else :\n","              new_x = path1[-1][0] + random.uniform(-1,1)\n","              new_y = path1[-1][1] + random.uniform(25,35)\n","              path1.append([new_x,new_y, round(0.1 * (i+1),1)])\n","        strokes.append({\"stroke_id\": 1, \"path\": path1})\n","\n","        # ë‘ë²ˆì§¸ íš\n","        path2 = []\n","        for i in range(5):\n","          if i == 0 :\n","            path2.append([x_start, y_start - 20, 0.1])\n","          else :\n","            path2.append([path2[-1][0] + random.uniform(3,6), path2[-1][1] + random.uniform(-1, 1), round(0.1 * (i + 1), 1)])\n","        strokes.append({\"stroke_id\": 2, \"path\": path2})\n","\n","    # ì˜ëª»ëœ case3 (3íš)\n","    elif case_type == \"incorrect_4\":  # ì˜ëª»ëœ ã…‚ (3íš)\n","        x_start = random.uniform(10, 60)\n","        y_start = random.uniform(60, 100)\n","\n","        #ì²«ë²ˆì§¸ íš\n","        path1 = []\n","        for i in range(5):\n","          if i == 0:\n","            path1.append([x_start, y_start, 0.1])  # ì²« ë²ˆì§¸ ì  t=0.1 ê°•ì œ ì„¤ì •\n","          else:\n","            new_x = path1[-1][0] + random.uniform(-1, 1) if i < 3 else path1[-1][0] + random.uniform(5, 15)\n","            new_y = path1[-1][1] - random.uniform(10, 19) if i < 3 else path1[-1][1] + random.uniform(-1, 1)\n","            new_t = round(i * 0.1 + 0.1, 1)  # 0.1, 0.2, 0.3, 0.4, 0.5 ë³´ì¥\n","            path1.append([new_x, new_y, new_t])\n","        strokes.append({\"stroke_id\": 1, \"path\": path1})\n","\n","        #ë‘ë²ˆì§¸ íš\n","        path2 = []\n","        for i in range(5):\n","          if i == 0 :\n","            path2.append([path1[-1][0], y_start, 0.1])\n","          else :\n","            path2.append([path2[-1][0] + random.uniform(-1, 1), path2[-1][1] - random.uniform(10, 19), round(0.1 * (i + 1), 1)])\n","        strokes.append({\"stroke_id\": 2, \"path\": path2})\n","\n","        #ì„¸ë²ˆì§¸ íš\n","        path3 = []\n","        for i in range(5) :\n","          if i == 0 :\n","            path3.append([x_start, y_start-20, 0.1])\n","          else:\n","            path3.append([path3[-1][0] + random.uniform(2,6), path3[-1][1] + random.uniform(-1,1) , round(0.1 * (i + 1), 1)])\n","        strokes.append({\"stroke_id\" : 3, \"path\": path3})\n","\n","    data.append({\n","        \"character\": \"ã…‚\",\n","        \"strokes\": strokes,\n","        \"label\": \"correct\" if case_type == \"correct\" else \"incorrect\"\n","    })\n","\n","# JSON íŒŒì¼ ì €ì¥\n","data_filename = \"generated_ã…‚_dataset.json\"\n","with open(data_filename, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(data, f, ensure_ascii=False, indent=4)\n","\n","files.download(data_filename)\n","print(f\"íŒŒì¼ {data_filename} ìƒì„± ì™„ë£Œ!\")"]},{"cell_type":"code","source":["import json\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Masking, Bidirectional\n","from sklearn.model_selection import train_test_split\n","\n","# ğŸ“Œ 1ï¸âƒ£ JSON íŒŒì¼ ë¡œë“œ\n","file_path = \"/content/generated_ã…‚_dataset.json\"  # Colabì—ì„œ ì—…ë¡œë“œ í›„ ê²½ë¡œ ì§€ì •\n","with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","    data = json.load(f)\n","\n","# ğŸ“Œ 2ï¸âƒ£ ë°ì´í„°ì…‹ ë³€í™˜\n","X, y = [], []\n","\n","for sample in data:\n","    all_strokes = []\n","\n","    for stroke in sample[\"strokes\"]:\n","        if len(all_strokes) > 0:\n","            all_strokes.append([0, 0, -1])  # ğŸš€ stroke ê°„ êµ¬ë¶„ì„ ìœ„í•œ padding ì¶”ê°€\n","        all_strokes.extend(stroke[\"path\"])\n","\n","    X.append(np.array(all_strokes))  # (N, 3) í˜•íƒœ\n","    y.append(1 if sample[\"label\"] == \"correct\" else 0)  # correct: 1, incorrect: 0\n","\n","# ğŸ“Œ 3ï¸âƒ£ Padding: LSTM ì…ë ¥ í¬ê¸° í†µì¼\n","max_length = max(len(p) for p in X)  # ê°€ì¥ ê¸´ sequence ì°¾ê¸°\n","X_padded = np.zeros((len(X), max_length, 3))  # (ë°ì´í„° ê°œìˆ˜, ìµœëŒ€ ê¸¸ì´, 3)\n","for i, path in enumerate(X):\n","    X_padded[i, :len(path), :] = path  # íŒ¨ë”© ì ìš©\n","\n","# ğŸ“Œ 4ï¸âƒ£ Independent Min-Max Normalization\n","def min_max_normalize(data):\n","    for i in range(2):  # x, y ì¢Œí‘œ ì •ê·œí™”\n","        min_val = np.min(data[:, :, i])\n","        max_val = np.max(data[:, :, i])\n","        data[:, :, i] = (data[:, :, i] - min_val) / (max_val - min_val + 1e-8)  # ì‘ì€ ê°’ ì¶”ê°€í•˜ì—¬ 0-ë¶„ëª¨ ë°©ì§€\n","    return data\n","\n","X_padded = min_max_normalize(X_padded)\n","\n","# ğŸ“Œ 5ï¸âƒ£ Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_padded, np.array(y), test_size=0.2, random_state=42)\n","\n","# ğŸ“Œ 6ï¸âƒ£ LSTM ëª¨ë¸ ì •ì˜\n","model = Sequential([\n","    Masking(mask_value=-1, input_shape=(max_length, 3)),  # ğŸš€ -1 ê°’ì„ ë¬´ì‹œí•˜ë„ë¡ ìˆ˜ì •\n","    Bidirectional(LSTM(128, return_sequences=True)),\n","    LSTM(64, return_sequences=True),\n","    LSTM(32),\n","    Dense(16, activation=\"relu\"),\n","    Dense(1, activation=\"sigmoid\")\n","])\n","\n","model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","\n","# ğŸ“Œ 7ï¸âƒ£ ëª¨ë¸ í•™ìŠµ\n","model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n","\n","# ğŸ“Œ 8ï¸âƒ£ ëª¨ë¸ ì €ì¥\n","model.save(\"/content/drive/MyDrive/lstm_model.h5\")\n","np.save(\"/content/drive/MyDrive/max_length.npy\", max_length)  # max_lengthë„ ì €ì¥\n","\n","print(\"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ë° ì €ì¥ë¨!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cBwZydHHuTe","executionInfo":{"status":"ok","timestamp":1739937135189,"user_tz":-540,"elapsed":392479,"user":{"displayName":"â€ì„œì •í›ˆ[í•™ìƒ](ê³µê³¼ëŒ€í•™ ì‚°ì—…ê²½ì˜ê³µí•™ê³¼)","userId":"13642878406273598628"}},"outputId":"4199bdfa-78ea-4d60-abed-a519b78944e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 107ms/step - accuracy: 0.9626 - loss: 0.0849 - val_accuracy: 1.0000 - val_loss: 1.3486e-04\n","Epoch 2/5\n","\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 1.0049e-04 - val_accuracy: 1.0000 - val_loss: 4.3952e-05\n","Epoch 3/5\n","\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 109ms/step - accuracy: 1.0000 - loss: 3.6555e-05 - val_accuracy: 1.0000 - val_loss: 2.1310e-05\n","Epoch 4/5\n","\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 1.8560e-05 - val_accuracy: 1.0000 - val_loss: 1.2204e-05\n","Epoch 5/5\n","\u001b[1m500/500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 1.0900e-05 - val_accuracy: 1.0000 - val_loss: 7.6589e-06\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ë° ì €ì¥ë¨!\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","\n","# ğŸ“Œ 1ï¸âƒ£ ëª¨ë¸ ë° max_length ë¶ˆëŸ¬ì˜¤ê¸°\n","model_path = \"/content/drive/MyDrive/lstm_model.h5\"\n","max_length_path = \"/content/drive/MyDrive/max_length.npy\"\n","\n","model = load_model(model_path)\n","max_length = int(np.load(max_length_path))\n","\n","print(\"âœ… ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!\")\n","\n","# ğŸ“Œ 3ï¸âƒ£ ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜\n","def predict_sequence(input_strokes):\n","    input_path = []\n","\n","    for stroke in input_strokes:\n","        input_path.extend(stroke[\"path\"])  # ëª¨ë“  stroke í•©ì¹˜ê¸°\n","\n","    input_path = np.array(input_path)\n","\n","    # Padding\n","    input_padded = np.zeros((1, max_length, 3))\n","    input_padded[0, :len(input_path), :] = input_path\n","\n","    # Independent Min-Max Normalization (for single input)\n","    for i in range(2):\n","        min_val = np.min(input_padded[:, :, i])\n","        max_val = np.max(input_padded[:, :, i])\n","        input_padded[:, :, i] = (input_padded[:, :, i] - min_val) / (max_val - min_val + 1e-8)\n","\n","    # Prediction\n","    pred = model.predict(input_padded)\n","    return \"correct\" if pred[0][0] > 0.5 else \"incorrect\", pred\n","\n","\n","# ğŸ“Œ ğŸ”¥ ì˜ˆì œ í…ŒìŠ¤íŠ¸\n","test_sample = {\n","\"strokes\": [\n","            {\n","                \"stroke_id\": 1,\n","                \"path\": [\n","                    [\n","                        19.111299287389123,\n","                        98.13772900208774,\n","                        0.1\n","                    ],\n","                    [\n","                        18.385977322061056,\n","                        85.50457000639261,\n","                        0.2\n","                    ],\n","                    [\n","                        18.417693304846253,\n","                        75.41294384925698,\n","                        0.3\n","                    ],\n","                    [\n","                        18.205275401987493,\n","                        60.53203913354896,\n","                        0.4\n","                    ],\n","                    [\n","                        17.822214339446873,\n","                        50.02450571707516,\n","                        0.5\n","                    ]\n","                ]\n","            },\n","            {\n","                \"stroke_id\": 2,\n","                \"path\": [\n","                    [\n","                        34.11129928738912,\n","                        98.13772900208774,\n","                        0.1\n","                    ],\n","                    [\n","                        34.49403385988232,\n","                        86.36460489819932,\n","                        0.2\n","                    ],\n","                    [\n","                        34.56357712024271,\n","                        73.52862716441356,\n","                        0.3\n","                    ],\n","                    [\n","                        34.2128758343882,\n","                        63.49409752646281,\n","                        0.4\n","                    ],\n","                    [\n","                        35.07327255776187,\n","                        53.38966238184945,\n","                        0.5\n","                    ]\n","                ]\n","            },\n","            {\n","                \"stroke_id\": 3,\n","                \"path\": [\n","                    [\n","                        19.111299287389123,\n","                        78.13772900208774,\n","                        0.1\n","                    ],\n","                    [\n","                        30.380617023639566,\n","                        79.0902255022847,\n","                        0.2\n","                    ],\n","                    [\n","                        45.05925122079077,\n","                        79.02451477804165,\n","                        0.3\n","                    ],\n","                    [\n","                        55.549897332782,\n","                        79.99927749807166,\n","                        0.4\n","                    ],\n","                    [\n","                        66.80247082605311,\n","                        79.02532514321831,\n","                        0.5\n","                    ]\n","                ]\n","            },\n","            {\n","                \"stroke_id\": 4,\n","                \"path\": [\n","                    [\n","                        19.111299287389123,\n","                        50.02450571707516,\n","                        0.1\n","                    ],\n","                    [\n","                        31.015799403591977,\n","                        50.29045849116906,\n","                        0.2\n","                    ],\n","                    [\n","                        44.576899229265116,\n","                        51.14660858370291,\n","                        0.3\n","                    ],\n","                    [\n","                        56.71894444949386,\n","                        50.74738910333353,\n","                        0.4\n","                    ],\n","                    [\n","                        69.05342107600336,\n","                        50.619682720317975,\n","                        0.5\n","                    ]\n","                ]\n","            }\n","]\n","}\n","\n","\n","result_label, pred_value = predict_sequence(test_sample[\"strokes\"])\n","print(\"Predicted result:\", result_label)\n","print(\"Prediction probability:\", pred_value)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P1mw3iqDnM9z","executionInfo":{"status":"ok","timestamp":1739937341658,"user_tz":-540,"elapsed":1677,"user":{"displayName":"â€ì„œì •í›ˆ[í•™ìƒ](ê³µê³¼ëŒ€í•™ ì‚°ì—…ê²½ì˜ê³µí•™ê³¼)","userId":"13642878406273598628"}},"outputId":"0d2e3b91-192f-4292-8cf8-fa6f0f31e39f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["âœ… ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!\n","\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790ms/step\n","Predicted result: correct\n","Prediction probability: [[0.9999391]]\n"]}]},{"cell_type":"code","source":["}"],"metadata":{"id":"6Z2m5ewxoVzl"},"execution_count":null,"outputs":[]}]}